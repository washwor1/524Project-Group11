{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a788780-66c4-45f0-bb89-6ebbdb6322c7",
   "metadata": {},
   "source": [
    "# Experiment Notebook\n",
    "\n",
    "This notebook will contain the steps to run the experiments for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e718b72d-1bdb-497f-a6b1-861cc39608fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 18:36:06.173061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732214166.343111   27413 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732214166.389210   27413 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 18:36:06.814820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import time\n",
    "import random \n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from modeling import rf, svm, lstm, gaussian\n",
    "from feature_engineering import extract_features\n",
    "from dataset_handling import book_train_test_split, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013183da-e400-42ae-b48c-eee3c9a7f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 21:18:46.779217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731964726.796679   79142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731964726.802108   79142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 21:18:46.823079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "LOGGER_NAME = \"proj2_logger\"\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "    def create_features(self, df: pd.DataFrame):\n",
    "        raise NotImplementedError(\"Function was not implemented in subclass\")\n",
    "    def fit(self) -> None:\n",
    "        raise NotImplementedError(\"Function was not implemented in subclass\")\n",
    "    def predict(self) -> []:\n",
    "        '''\n",
    "        Run the model against the test partition of the dataset.\n",
    "\n",
    "        Returns metrics: Time, Accuracy, F1-Score, Precision, Recall\n",
    "        '''\n",
    "        raise NotImplementedError(\"Function was not implemented in subclass\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    def create_features(self, df: pd.DataFrame):\n",
    "        # split df into pre-created train-test groups\n",
    "        self.num_labels = len(df.author_id.unique())\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        max_len = 128\n",
    "        dataset = CustomDataset(df.text, df.author_id, tokenizer, max_len)\n",
    "        train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    def fit(self):\n",
    "        # fit transformer\n",
    "        self.start_time = time.time()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=self.num_labels) \n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training\n",
    "        epochs = 3\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "        \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return None\n",
    "    def predict() -> []:\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "        \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, axis=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        \n",
    "        # Metric\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        \n",
    "        return [['transformer', 'embeddings', 'test', duration, accuracy, f1, precision, recall]]\n",
    "\n",
    "class ClassicalModels(Model):\n",
    "    def create_features(self, df: pd.DataFrame):\n",
    "        self.tfidf, self.embeddings = extract_features(df)\n",
    "        self.labels = df['author_id']\n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "        To avoid reusing code, this function does nothing, as\n",
    "        the per-model functions already train and then test\n",
    "        '''\n",
    "        return None\n",
    "    \n",
    "    def predict(self):\n",
    "        # run all models and return metrics\n",
    "        functions = [rf, gaussian, svm, lstm]\n",
    "        metrics_arr = []\n",
    "        for feature_type in ['glove', 'tfidf']:\n",
    "            self.logger.debug(f\"Processing {feature_type} features\")\n",
    "            features = self.tfidf if feature_type == \"tfidf\" else self.embeddings\n",
    "            for function in functions:\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    self.logger.debug(f\"Beginning testing of {function.__name__} with {feature_type} features\")\n",
    "                    metrics, classification_report, pr = function(features, self.labels)\n",
    "                    self.logger.debug(classification_report)\n",
    "                    self.logger.debug(f\"Finished testing of {function.__name__} with {feature_type} features (took {time.time() - start_time} seconds)\")\n",
    "                    metrics_arr.append([function.__name__, feature_type, 'test', *metrics]\n",
    "                except Exception as e:\n",
    "                    print(e)  \n",
    "            self.logger.debug(f\"Finished processing {feature_type} features\")\n",
    "        return metrics_arr\n",
    "\n",
    "def experiment(datafile_path='data/dataset.parquet'):\n",
    "    # load dataset\n",
    "    # run train-test-split on df (will produce label column)\n",
    "    df = book_train_test_split(load_dataset(datafile_path))\n",
    "    models = [TransformerModel(), ClassicalModels()]\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        model.create_features(df)\n",
    "        model.fit()\n",
    "        metrics += model.predict()\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['model_name', 'data_type', 'phase', 'time', 'accuracy'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b96a48a-e018-423e-b64c-a34f135b11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = book_train_test_split(load_dataset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc59390-f8ab-4bff-a756-1481c4487842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = book_train_test_split(load_dataset(\"data/primary_authors_dataset.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0d8b46-d5e1-4eca-8783-5ca6c395349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already exist.\n",
      "Loaded 2196008 word vectors.\n",
      "Average Number of Words not in Embedding Vocab: 3.4205376400652323\n",
      "Embeddings saved to document_embeddings.npy\n",
      "Computing TF-IDF scores...\n",
      "Average Number of Words not in Embedding Vocab: 955.8599084644115\n",
      "Embeddings saved to document_embeddings_tfidf.npy\n",
      "Extracting TF-IDF features...\n"
     ]
    }
   ],
   "source": [
    "tfidf, vecs = extract_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655c6680-cd70-4d39-a70f-c3245bfbc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.to_csv(\"data/primary_authors_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52ebbf-48c4-4368-8af1-4531148649cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already exist.\n",
      "Loaded 2196008 word vectors.\n",
      "Average Number of Words not in Embedding Vocab: 5.076604691522193\n",
      "Embeddings saved to document_embeddings.npy\n",
      "Computing TF-IDF scores...\n"
     ]
    }
   ],
   "source": [
    "tfidf, vecs = extract_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ed5776-1147-4f34-b59c-992db80ced6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4a2112-e329-4e7f-9949-b4b5d6873399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([2., 2., 2., 2., 2., 2., 2., 2., 2.]), array([3., 3., 3., 3., 3., 3., 3., 3., 3.]), array([4., 4., 4., 4., 4., 4., 4., 4., 4.]), array([5., 5., 5., 5., 5., 5., 5., 5., 5.]), array([6., 6., 6., 6., 6., 6., 6., 6., 6.]), array([7., 7., 7., 7., 7., 7., 7., 7., 7.]), array([8., 8., 8., 8., 8., 8., 8., 8., 8.]), array([9., 9., 9., 9., 9., 9., 9., 9., 9.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 5., 5., 5., 5., 5., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [i * np.ones(shape=9) for i in range(1, 10)]\n",
    "print(arr)\n",
    "np.mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4053f9-9c93-4505-b581-9644a7bcc13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9) / 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
